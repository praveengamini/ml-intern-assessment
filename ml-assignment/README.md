# Trigram Language Model

This directory contains the core assignment files for the Trigram Language Model (Task 1) and the optional Scaled Dot-Product Attention module (Task 2).

## ğŸ“¦ Installation

Make sure you have Python 3.8+ installed, then install dependencies:

```
pip install -r requirements.txt
```

If you created a virtual environment:

```
source venv/bin/activate
# or on Windows:
venv\Scripts\activate
```

## â–¶ï¸ How to Run the Model

### 1. Train and Generate Text

Run the main script:

```
python src/generate.py
```

This will:
* Read the example corpus from `data/example_corpus.txt`
* Train the trigram model
* Print generated text to the console

## ğŸ§ª Running Tests

To run all unit tests:

```
pytest -q
```

This validates:
* Trigram training
* Text generation
* Empty/short text handling

If pytest cannot find the `src` folder, set PYTHONPATH:

```
export PYTHONPATH="."
pytest -q
```

(Usually not required.)

## ğŸ“˜ Project Structure

```
ml-assignment/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ngram_model.py        # Trigram model implementation
â”‚   â”œâ”€â”€ generate.py           # Script to run training + text generation
â”‚   â”œâ”€â”€ utils.py              # Optional helper utilities
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ngram.py         # Unit tests for trigram model
â”‚   â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ example_corpus.txt    # Sample training text
â”‚
â”œâ”€â”€ attention/ (Task 2 - optional)
â”‚   â”œâ”€â”€ scaled_attention.py   # Numpy-only scaled dot-product attention
â”‚   â””â”€â”€ demo.py               # Demonstration script
â”‚
â”œâ”€â”€ evaluation.md             # Explanation of design choices
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ§  Evaluation Document

All design decisionsâ€”including:
* Text cleaning
* Padding
* N-gram storage
* Probability sampling
* Efficiency improvements
* Task 2 details

are described in evaluation.md.

## âœ”ï¸ Optional Task 2: Scaled Dot-Product Attention

To run the Task-2 demo:

```
python attention/demo.py
```

This uses the NumPy-only implementation in:

```
attention/scaled_attention.py
```